{"courses":[{"id":1,"courseTitle":"Computer Vision","author":"AWS Machine Learning University","courseBanner":"https://images.unsplash.com/photo-1501621667575-af81f1f0bacc?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1170&q=80","videos":[{"id":1,"videoTitle":"Accelerated Computer Vision 1.1 - Intro","link":"https://www.youtube.com/watch?v=_6CFi2CO2AI&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta","description":"In this video, we go over this course overview, learning outcomes and machine learning resources that we will use in this class.","youtubeID":"_6CFi2CO2AI"},{"id":2,"videoTitle":"Accelerated Computer Vision 1.2 - Introduction to Machine Learning","link":"https://www.youtube.com/watch?v=mwNtbKh39SA&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=3","description":"In this video, we make an introduction to Machine Learning (ML). We learn machine learning lifecycle that gives us a high level view of some important ML processes and cover some useful ML terminology.","youtubeID":"mwNtbKh39SA"},{"id":3,"videoTitle":"Accelerated Computer Vision 1.3 - ML Applications","link":"https://www.youtube.com/watch?v=UM4a1quelGk&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=3","description":"In this video, we give some commonly seen ML application examples. We go over ranking, recommendation, classification, regression, clustering and anomaly detection applications and see some examples.","youtubeID":"UM4a1quelGk"},{"id":4,"videoTitle":"Accelerated Computer Vision 1.4 - Supervised and Unsupervised Learning","link":"https://www.youtube.com/watch?v=yCm5NmBADD0&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=4","description":"In this video, we learn supervised and unsupervised learning. We then dive into more details of supervised learning with regression and classification problems, and a clustering example for unsupervised learning.","youtubeID":"yCm5NmBADD0"},{"id":5,"videoTitle":"Accelerated Computer Vision 1.5 - Data Processing - Imbalanced Data","link":"https://www.youtube.com/watch?v=lKN076dleoA&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=5","description":"In this video, we learn some data processing methods. We first start with the data imbalance problem and go over some methods to solve it. We then dive into the details about image augmentation methods to produce some altered images. At the end, we learn how to split our dataset into training, validation and test subsets which is a critical process in ML.","youtubeID":"lKN076dleoA"},{"id":6,"videoTitle":"Accelerated Computer Vision 1.6 - Underfitting, Overfitting and Model Evaluation","link":"https://www.youtube.com/watch?v=VRdDB3J1_2k&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=6","description":"In this video, we learn how to find out whether our ML models might be underfitting or overfitting. We also go over important model evaluation metrics for regression and classification problems. For regression, we have Mean Square Error (MSE), Rooted Mean Square Error (RMSE), Mean Absolute Error (MAE) and R Squared (R2) metrics. For classification, we introduce confusion matrix and learn Accuracy, Precision & Recall and F1 score metrics.","youtubeID":"VRdDB3J1_2k"},{"id":7,"videoTitle":"Accelerated Computer Vision 1.7 - Computer Vision Applications","link":"https://www.youtube.com/watch?v=HmiB0RbdwTk&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=7","description":"In this video, we get familiar with some main computer vision applications. We learn image classification, object detection and semantic segmentation tasks.","youtubeID":"HmiB0RbdwTk"},{"id":8,"videoTitle":"Accelerated Computer Vision 1.8 - Image Representation","link":"https://www.youtube.com/watch?v=o6xH2kmBda0&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=8","description":"In this video, we learn how images are represented in our machine learning programs. We then see dive into two types of images: Gray scale images and color images.","youtubeID":"o6xH2kmBda0"},{"id":9,"videoTitle":"Accelerated Computer Vision 1.9 - Neuron & Activation Functions","link":"https://www.youtube.com/watch?v=k-1rohIhR1A&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=9","description":"In this video, we learn an important part of neural networks: Artificial neurons and their activation functions. We learn how artificial neurons work and cover some well-known activation functions: Sigmoid, Hyperbolic Tangent (tanh) and Rectified Linear Unit (ReLU).","youtubeID":"k-1rohIhR1A"},{"id":10,"videoTitle":"Accelerated Computer Vision 1.10 - Neural Networks: Components and Training","link":"https://www.youtube.com/watch?v=tbKCR1v93ok&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=10","description":"In this video, we dive into deep learning (or often called Neural Networks). We learn the layered structure of neural networks, how data is passed with forward propagation and how training (backpropagation) works. We then see more details on training with the gradient descent method and how to pick up a learning rate.","youtubeID":"tbKCR1v93ok"},{"id":11,"videoTitle":"Accelerated Computer Vision 1.11 - Convolutions (Filters)","link":"https://www.youtube.com/watch?v=JEEUbAzwmuI&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=11","description":"In this video, we learn what convolutions (often called filters or kernels) are and how we can derive them by translation invariance and locality of human vision system. We then see how to integrate convolutions in to neural networks.","youtubeID":"JEEUbAzwmuI"},{"id":12,"videoTitle":"Accelerated Computer Vision 1.12 - Padding, Stride and Pooling","link":"https://www.youtube.com/watch?v=8gybwqgB_L8&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=12","description":"In this video, we dive into more details about convolution operations with padding and stride. We then learn pooling layer that allows us to downsample our feature maps. At the end, we see a simple CNN made of convolution, pooling and dense layers.","youtubeID":"8gybwqgB_L8"},{"id":13,"videoTitle":"Using Jupyter Notebooks on Sagemaker","link":"https://www.youtube.com/watch?v=-DKydfPFqBM&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=13","description":"In this video, we learn how to run Jupyter notebooks on Sagemaker. GitHub repositories: Accelarated NLP ,  Accelarated CV, Accelarated TAB","youtubeID":"-DKydfPFqBM"},{"id":14,"videoTitle":"Accelerated Computer Vision 2.1 - Computer Vision Datasets","link":"https://www.youtube.com/watch?v=6ACoEFKKd3g&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=14","description":"We get familiar with some well-known computer vision datasets in this video. We go over MNIST, Fashion-MNIST, CIFAR-10, ImageNet, Places and Open Images datasets and how we can utilize these datasets for a various of compute vision applications.","youtubeID":"6ACoEFKKd3g"},{"id":15,"videoTitle":"Accelerated Computer Vision 2.2 - LeNet","link":"https://www.youtube.com/watch?v=EjYsirSi0No&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=15","description":"In this video, we learn one of the earliest successful CNN architectures: LeNet. We explain its architectural details and visualize how it applies on the real-world MNIST dataset.","youtubeID":"EjYsirSi0No"},{"id":16,"videoTitle":"Accelerated Computer Vision 2.3 - AlexNet","link":"https://www.youtube.com/watch?v=Kti3fRE5IHs&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=16","description":"In this video, we learn one of the most important CNN models: AlexNet. We go over the architecture design of AlexNet, and understand which architecture components boost its performance by comparing with LeNet which we learned in the previous video.","youtubeID":"Kti3fRE5IHs"},{"id":17,"videoTitle":"Accelerated Computer Vision 2.4 - Transfer Learning","link":"https://www.youtube.com/watch?v=4ZLMsbYe6QQ&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=17","description":"In this video, we learn the transfer learning concept that enables us to save time and computational cost. By using some pretrained models instead of training from scratch, transfer learning allows us utilizing knowledge gained from other ML models. We understand a various of methods to apply transfer learning and a popular pretrained model zoo - GluonCV.","youtubeID":"4ZLMsbYe6QQ"},{"id":18,"videoTitle":"Accelerated Computer Vision 3.1 - VGG and Batch Normalization","link":"https://www.youtube.com/watch?v=IhMW2agL4Js&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=18","description":"In this video, we dive into the VGG model. First, we understand its architecture and point out differences from the previously learned model AlexNet. We then learn batch normalization that normalizes inputs of each layers of neural networks and improves speed and training convergence of networks.","youtubeID":"IhMW2agL4Js"},{"id":19,"videoTitle":"Accelerated Computer Vision 3.2 - ResNet","link":"https://www.youtube.com/watch?v=4FFXCH7eSS4&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=19","description":"In this video, we first talk about the degradation problem resulted from adding more layers to neural networks. Then, we learn the ResNet model through the residual connection concept and understand how it solves the degradation problem.","youtubeID":"4FFXCH7eSS4"},{"id":20,"videoTitle":"Accelerated Computer Vision 3.3 - Object Detection Applications","link":"https://www.youtube.com/watch?v=Xf-5r9MghYg&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=20","description":"In this video, we see some example applications of object detection.","youtubeID":"Xf-5r9MghYg"},{"id":21,"videoTitle":"Accelerated Computer Vision 3.4 - Bounding Box and Anchor Bo","link":"https://www.youtube.com/watch?v=_7s0IRD7OQs&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=21","description":"In this video, we introduce bounding box which is important for object detection problems. We then go over Intersection over Union (IoU) concept to quantify goodness of our proposed bounding boxes. Besides, we talk about the issues of bounding box (such as occlusion) and the solution - anchor box.","youtubeID":"_7s0IRD7OQs"},{"id":22,"videoTitle":"Accelerated Computer Vision 3.5 - Sliding Window Method and Non-max Suppression","link":"https://www.youtube.com/watch?v=ehs7plKpfE8&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=22","description":"In this video, we learn how to best utilize bounding box: the sliding window method and Non-Max Suppression (NMS). With NMS, we refine the proposed bounding boxes by removing some of them using overlap and confidence scores.","youtubeID":"ehs7plKpfE8"},{"id":23,"videoTitle":"Accelerated Computer Vision 3.6 - Region Based Convolutional Neural Networks (R-CNNs)","link":"https://www.youtube.com/watch?v=Z9nCBtaEb_g&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=23","description":"In this video, we introduce an object detection method: Region Based Convolutional Neural Networks (R-CNNs) and explain its selective search, feature extraction and prediction steps. We then go over two other models that improved upon the R-CNN model: Fast R-CNN and Faster R-CNN.","youtubeID":"Z9nCBtaEb_g"},{"id":24,"videoTitle":"Accelerated Computer Vision 3.7 - You Only Look Once (YOLO) model","link":"https://www.youtube.com/watch?v=NfLycjxyEww&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=25","description":"In this video, we cover another object detection model: You Only Look Once (YOLO). We see that this model introduces a grid-like structure and uses an object detection network that can be easily trained end-to-end. We also go over the output shape of the network that depends on how many bounding boxes produced and the number of classes.","youtubeID":"NfLycjxyEww"},{"id":25,"videoTitle":"Accelerated Computer Vision 3.8 - Semantic Segmentation","link":"https://www.youtube.com/watch?v=yHVfQWNrLQo&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=26","description":"In this video, we learn the semantic segmentation concept and go over some example datasets.","youtubeID":"yHVfQWNrLQo"},{"id":26,"videoTitle":"Accelerated Computer Vision 3.9 - Fully Convolutional Networks","link":"https://www.youtube.com/watch?v=nph6SI8KxTM&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=24","description":"In this video, we go over our first semantic segmentation network - Fully Convolutional Networks (FCNs). We learn the important components of FCNs, such as transposed convolution and skip connections.","youtubeID":"nph6SI8KxTM"},{"id":27,"videoTitle":"Accelerated Computer Vision 3.10 - U-Net","link":"https://www.youtube.com/watch?v=PXJI6JcQB34&list=PL8P_Z6C4GcuU4knhhCouJujFZ2tTqU-Ta&index=27","description":"In this video, we learn our last network of this course: U-Net, and compare it with the Fully Convolutional Networks (FCNs).","youtubeID":"PXJI6JcQB34"}]},{"id":2,"courseTitle":"Natural Language Processing (NLP)","author":"AWS Machine Learning University","courseBanner":"https://images.unsplash.com/photo-1592431913823-7af6b323da9b?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1170&q=80","videos":[{"id":1,"videoTitle":"Accelerated Natural Language Processing 1.1 - Course Introduction","link":"https://www.youtube.com/watch?v=0FXKbEgz-uU&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw","description":"In this video, we go over the course overview, learning outcomes and machine learning resources that we will use in this class.","youtubeID":"0FXKbEgz-uU"},{"id":2,"videoTitle":"Accelerated Natural Language Processing 1.2 - Introduction to Machine Learning","link":"https://www.youtube.com/watch?v=qpGGR9e1_Fo&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=2","description":"In this video, we make an introduction to Machine Learning (ML). We learn machine learning lifecycle that gives us a high level view of some important ML processes and cover some useful ML keywords.","youtubeID":"qpGGR9e1_Fo"},{"id":3,"videoTitle":"Accelerated Natural Language Processing 1.3 - ML Applications","link":"https://www.youtube.com/watch?v=D6pMnwjj6AU&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=3","description":"We give some ML application examples in this video. We go over ranking, recommendation, classification, regression, clustering and anomaly detection applications and see some examples","youtubeID":"D6pMnwjj6AU"},{"id":4,"videoTitle":"Accelerated Natural Language Processing 1.4 - Supervised and Unsupervised Learning","link":"https://www.youtube.com/watch?v=lXHyikKT_hw&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=4","description":"In this video, we learn supervised and unsupervised learning. We then see more details on supervised learning with regression and classification example problems and also have an example for unsupervised learning.","youtubeID":"lXHyikKT_hw"},{"id":5,"videoTitle":"Accelerated Natural Language Processing 1.5 - Class Imbalance","link":"https://www.youtube.com/watch?v=GvK_8UXStOo&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=5","description":"We look at the class imbalance problem in this video and go over some methods to fix it.","youtubeID":"GvK_8UXStOo"},{"id":6,"videoTitle":"Accelerated Natural Language Processing 1.6 - Missing Values","link":"https://www.youtube.com/watch?v=MvbYoMHoou4&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=6","description":"In this video, we go over missing values topic which is a common issue in machine learning problems. We learn to solve this problem by dropping records or by using some imputation methods.","youtubeID":"MvbYoMHoou4"},{"id":7,"videoTitle":"Accelerated Natural Language Processing 1.7 - Model Evaluation","link":"https://www.youtube.com/watch?v=2MXtSJz5e0g&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=7","description":"In this video, we learn model evaluation metrics, dataset splitting and overfitting-underfitting cases. For model evaluation, we consider regression and classification specific metrics. We then explain the dataset splitting process that allows us to get training, validation and test sets. We also talk about how to find out whether our ML models might be underfitting or overfitting.","youtubeID":"2MXtSJz5e0g"},{"id":8,"videoTitle":"Accelerated Natural Language Processing 1.8 - Introduction to NLP","link":"https://www.youtube.com/watch?v=xUxw6C_z2kA&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=8","description":"We make introduction to NLP and learn these NLP terms: Corpus, Token and Feature vector in this video.","youtubeID":"xUxw6C_z2kA"},{"id":9,"videoTitle":"Accelerated Natural Language Processing 1.9 - Machine Learning and Text","link":"https://www.youtube.com/watch?v=cAf9J9dR4u4&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=9","description":"In this video, we learn a simple pipeline to process text information. Machine learning models need well defined numerical data. Our pipeline takes text data and applies pre-processing operations on it. Then, it converts it into numerical representation with the vectorization step. After that, we can use a simple machine learning model on the data.","youtubeID":"cAf9J9dR4u4"},{"id":10,"videoTitle":"Accelerated Natural Language Processing 1.10 - Text Preprocessing","link":"https://www.youtube.com/watch?v=BERgPsvyESs&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=10","description":"We give more details about text pre-processing in this video. We learn tokenization, stop words removal and stemming-lemmatization methods. These methods will clean-up and normalize our text data.","youtubeID":"BERgPsvyESs"},{"id":11,"videoTitle":"Accelerated Natural Language Processing 1.11 - Text Vectorization","link":"https://www.youtube.com/watch?v=TcLlCNmiD3Y&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=11","description":"In this video, we learn how to convert text data into numerical representation using Bag of Words method. This is an easy method that uses word counts/frequencies and gets numerical features from text data. We go over word counts, Term Frequency (TF) and Term Frequency - Inverse Document Frequency (TF-IDF).","youtubeID":"TcLlCNmiD3Y"},{"id":12,"videoTitle":"Accelerated Natural Language Processing 1.12 - K Nearest Neighbors","link":"https://www.youtube.com/watch?v=Up1WhKPBlqM&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=12","description":"We introduce the K Nearest Neighbors model in this video. We first understand how this model works. We then learn how to choose the K parameter and use different distance metrics. We also learn how curse of dimensionality affects this model and why feature scaling helps getting better outcomes.","youtubeID":"Up1WhKPBlqM"},{"id":13,"videoTitle":"Using Jupyter Notebooks on Sagemaker","link":"https://www.youtube.com/watch?v=-DKydfPFqBM&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=13","description":"In this video, we learn how to run Jupyter notebooks on Sagemaker. GitHub repositories: Accelerated NLP,  Accelerated CV, Accelerated TAB","youtubeID":"-DKydfPFqBM"},{"id":14,"videoTitle":"Accelerated Natural Language Processing 2.1 - Tree-based Models","link":"https://www.youtube.com/watch?v=kvSWoAge8vs&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=14","description":"We understand the working principles of decision trees in this video. We fit a simple decision tree on a sample dataset. We then introduce the impurity concept with Gini impurity score and explain the split condition selection process. After decision tree discussion, we learn how to build ensemble models using Bagging approach.","youtubeID":"kvSWoAge8vs"},{"id":15,"videoTitle":"Accelerated Natural Language Processing 2.2 - Regression Models","link":"https://www.youtube.com/watch?v=sGy8yWq9O1g&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=15","description":"In this section, we learn two highly popular machine learning models: Linear and Logistic regression. We understand the equations and cost functions of each model.","youtubeID":"sGy8yWq9O1g"},{"id":16,"videoTitle":"Accelerated Natural Language Processing 2.3 - Optimization","link":"https://www.youtube.com/watch?v=sphTv3omQ6I&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=16","description":"In this video, we learn the basics of optimization topic which is key to training many machine learning models. We start with the gradient concept and then introduce gradient descent optimization technique.","youtubeID":"sphTv3omQ6I"},{"id":17,"videoTitle":"Accelerated Natural Language Processing 2.4 - Regularization","link":"https://www.youtube.com/watch?v=C6ZbAtr6agY&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=17","description":"In this video, we learn another key concept in machine learning: Regularization. With regularization we are able to get ML models that can generalize well on test sets. We learn L1, L2 and ElasticNet regularizations and also see the Sklearn interfaces for them.","youtubeID":"C6ZbAtr6agY"},{"id":18,"videoTitle":"Accelerated Natural Language Processing 2.5 - Hyperparameter Tuning","link":"https://www.youtube.com/watch?v=Zrx2PRU3J3k&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=18","description":"We learn how to find a good combination of hyperparameters using different approaches in this video. We cover grid search, randomized search and bayesian search.","youtubeID":"Zrx2PRU3J3k"},{"id":19,"videoTitle":"Accelerated Natural Language Processing 3.1 - Neural Networks","link":"https://www.youtube.com/watch?v=fW-9HunNYFQ&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=19","description":"In this video, we cover one of the most important modern machine learning models: Neural Network. We introduce neural networks using their similarity to the regression models that we learned. We learn the layered structure of neural networks, how data is passed with forward propagation and how training works. Then, we see more details on training process with cost functions and the gradient descent method.","youtubeID":"fW-9HunNYFQ"},{"id":20,"videoTitle":"Accelerated Natural Language Processing 3.2 - Word Vectors","link":"https://www.youtube.com/watch?v=BwsGoFDn06g&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=20","description":"In this video, we learn how to build embedding vectors for the words in our corpus. We also call these word vectors. With these vectors, we are able to capture semantic similarity between words. We use dot product to measure similarity between word vectors. We then see details of Skip-gram and Continuous Bag of Words (CBOW) methods and build a probabilistic model that gives word vectors.","youtubeID":"BwsGoFDn06g"},{"id":21,"videoTitle":"Accelerated Natural Language Processing 3.3 - Recurrent Neural Networks","link":"https://www.youtube.com/watch?v=mgq6-jwGtcA&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=21","description":"In this video, we learn a new type of neural network: Recurrent Neural Network (RNN). RNNs are usually used to process sequential data. We learn the overall structure of RNNs and understand how they use internal hidden states to preserve and update sequential information.","youtubeID":"mgq6-jwGtcA"},{"id":22,"videoTitle":"Accelerated Natural Language Processing 3.4 - Gated Recurrent Units (GRUs)","link":"https://www.youtube.com/watch?v=XjnLlBK64ZY&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=22","description":"In this video, we build on the idea of RNNs and introduce a new type of network. Gated Recurrent Units (GRUs) use internal gates to control hidden state updates. We introduce update and forget gates. Reset gate decides how much to remove from the previous hidden state. We also calculate a candidate hidden state. We use the update gate to decide how much we want to include this candidate in the overall hidden state.","youtubeID":"XjnLlBK64ZY"},{"id":23,"videoTitle":"Accelerated Natural Language Processing 3.5 - Long Short Term Memory (LSTM) Networks","link":"https://www.youtube.com/watch?v=Upz07pfaJm8&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=23","description":"Long Short Term Memory (LSTM) Networks use a similar approach like the Gated Recurrent Units (GRUs). In this video, we learn the internal pieces of LSTMs and understand how they work together to preserve and update sequential information.","youtubeID":"Upz07pfaJm8"},{"id":24,"videoTitle":"Accelerated Natural Language Processing 3.6 - Transformers","link":"https://www.youtube.com/watch?v=mV8c9dJpxQE&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=24","description":"In this video, we make an introduction to Transformers. We explain the key, value, query concepts using a linguistic approach.","youtubeID":"mV8c9dJpxQE"},{"id":25,"videoTitle":"Accelerated Natural Language Processing 3.7 - Single Headed Attention","link":"https://www.youtube.com/watch?v=flHtAUGB0PU&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=25","description":"In this video, we learn how to implement the query, key, value concepts using a neural network approach. We call this method \"Single Headed Attention\".","youtubeID":"flHtAUGB0PU"},{"id":26,"videoTitle":"Accelerated Natural Language Processing 3.8 - Multi Headed Attention","link":"https://www.youtube.com/watch?v=N9AGY-Z6tbM&list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&index=26","description":"In this video, we introduce Multi Headed Attention concept and conclude Transformers topic. We learn that Multi-headed attention helps to cover a broader range of word meanings. Then, we briefly go over Transformer architecture and give example models.","youtubeID":"N9AGY-Z6tbM"}]},{"id":3,"courseTitle":"Tabular Data","author":"AWS Machine Learning University","courseBanner":null,"videos":[{"id":1,"videoTitle":"Accelerated Tabular Data 1.1 - Course Introduction","link":"https://www.youtube.com/watch?v=kj-sPC6pai4&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2","description":"Machine Learning University Accelerated Tabular Data course content, learning outcomes, and Machine Learning resources for the class.","youtubeID":"kj-sPC6pai4"},{"id":2,"videoTitle":"Accelerated Tabular Data 1.2 - Introduction to Machine Learning","link":"https://www.youtube.com/watch?v=jiT8tCGKEVc&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&index=2","description":"This video introduces key Machine Learning concepts, goes over some Machine Learning applications, discusses Machine Learning life-cycle, and reviews useful Machine Learning terms. Supervised and unsupervised learning is discussed, with some classification, regression and clustering examples. A simple K Nearest Neighbors (KNN) model is build to handle a tabular data problem.","youtubeID":"jiT8tCGKEVc"},{"id":3,"videoTitle":"Accelerated Tabular Data 1.3 - Model Evaluation","link":"https://www.youtube.com/watch?v=JSOFsQQUDlU&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&index=3","description":"This video discusses Machine Learning model evaluation metrics, dataset splitting, along with underfitting and overfitting. For model evaluation, regression and classification specific metrics are discussed. Dataset splitting process is explained, allowing us to get training, validation, and test sets. We also explore using these datasets to find out whether the Machine Learning models might be underfitting or overfitting.","youtubeID":"JSOFsQQUDlU"},{"id":4,"videoTitle":"Accelerated Tabular Data 1.4 - Exploratory Data Analysis","link":"https://www.youtube.com/watch?v=lkHvVwLJoFw&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&index=4","description":"This video covers some basic Exploratory Data Analysis (EDA) tools and concepts. We learn how to use histograms, bar plots, scatter plots, correlation matrices, and understand the correlation concept. We also discuss techniques to preprocess imbalanced datasets and handle missing values.","youtubeID":"lkHvVwLJoFw"},{"id":5,"videoTitle":"Accelerated Tabular Data 1.5 - K Nearest Neighbors","link":"https://www.youtube.com/watch?v=Mt8uJw8i68o&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&index=5","description":"This video introduces the K Nearest Neighbors (KNN) algorithm. We first understand how the KNN algorithm works. We then discuss how to choose the K parameter and use different distance metrics to raise performance. We also explain how curse of dimensionality affects the algorithm, and why feature scaling improves outcomes.","youtubeID":"Mt8uJw8i68o"},{"id":6,"videoTitle":"Accelerated Tabular Data 1.6 - Looking Ahead","link":"https://www.youtube.com/watch?v=iM-QDJWVces&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&index=6","description":"This video briefly introduces the topics discussed in Lecture 2.","youtubeID":"iM-QDJWVces"},{"id":7,"videoTitle":"Using Jupyter Notebooks on Sagemaker","link":"https://www.youtube.com/watch?v=-DKydfPFqBM&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&index=7","description":"In this video, we learn how to run Jupyter notebooks on Sagemaker.","youtubeID":"-DKydfPFqBM"},{"id":8,"videoTitle":"Accelerated Tabular Data 2.1 - Introduction","link":"https://www.youtube.com/watch?v=NXLONchge5o&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&index=8","description":"This video goes over topics covered in Lecture 2, building on the previous lecture, and learning more advanced Machine Learning topics.","youtubeID":"NXLONchge5o"},{"id":9,"videoTitle":"Accelerated Tabular Data 2.2 - Feature Engineering","link":"https://www.youtube.com/watch?v=aDdNXDoQDYY&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&index=9","description":"This video discusses how to produce useful features from our raw data with Feature Engineering, with more details on handling two common types of data: Categorical and Text data. For categorical data, we explore ordinal encoding, one-hot encoding and target encoding. For text data, we use a text processing pipeline that involves text pre-processing/cleaning and vectorization.","youtubeID":"aDdNXDoQDYY"},{"id":10,"videoTitle":"Accelerated Tabular Data 2.3 - Tree-based Models","link":"https://www.youtube.com/watch?v=Y2pryO9BFEA&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&index=10","description":"This video first discusses the working principles of decision trees. We fit a simple decision tree on a sample dataset. We then introduce the impurity concept with Gini impurity score and explain the split condition selection process. We also learn how to build ensemble models using Bagging techniques.","youtubeID":"Y2pryO9BFEA"},{"id":11,"videoTitle":"Accelerated Tabular Data 2.4 - Hyperparameter Tuning","link":"https://www.youtube.com/watch?v=EY5r7ydFHRc&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&index=11","description":"This video discusses how to find a good combination of hyperparameters using different approaches. We cover grid search, randomized search and Bayesian search.","youtubeID":"EY5r7ydFHRc"},{"id":12,"videoTitle":"Accelerated Tabular Data 2.5 - AWS SageMaker","link":"https://www.youtube.com/watch?v=wt6dCrXDhdE&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&index=12","description":"This video briefly introduces AWS SageMaker, a fully managed service that removes the heavy lifting from each step of the Machine Learning process: building, training, tuning, and deploying Machine Learning models.","youtubeID":"wt6dCrXDhdE"},{"id":13,"videoTitle":"Accelerated Tabular Data 3.1 - Introduction","link":"https://www.youtube.com/watch?v=_fAU1_qCuPw&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&index=13","description":"This video goes over topics covered in Lecture 3, building on the previous lecture, and learning more advanced Machine Learning topics.","youtubeID":"_fAU1_qCuPw"},{"id":14,"videoTitle":"Accelerated Tabular Data 3.2 - Optimization, Regression Models and Regularization","link":"https://www.youtube.com/watch?v=2eBMsyxrtQI&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&index=14","description":"This video starts reviewing the basics of optimization, which is key to training many Machine Learning models. We introduce the gradient concept and the gradient descent optimization technique. We then discuss Linear and Logistic Regression models, with focus on learning these models by gradient descent. We also discuss another key concept in Machine Learning: Regularization. With regularization we are able to build Machine Learning models that can generalize well on unseen datasets. We cover the L1, L2, and ElasticNet regularization.","youtubeID":"2eBMsyxrtQI"},{"id":15,"videoTitle":"Accelerated Tabular Data 3.3 - Ensemble Methods: Boosting","link":"https://www.youtube.com/watch?v=bV1k3hmVGRU&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&index=15","description":"This video discusses Boosting, an ensemble method to create strong models by building multiple weak models sequentially. We briefly cover: gradient boosting machines, XGBoost, LightGBM, and CatBoost.","youtubeID":"bV1k3hmVGRU"},{"id":16,"videoTitle":"Accelerated Tabular Data 3.4 - Neural Networks and AutoML","link":"https://www.youtube.com/watch?v=btxc0DcsUxQ&list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&index=16","description":"This video covers one of the most important type of modern Machine Learning models: Neural Networks. We introduce neural networks using their similarity to regression models. We learn about the layered structure of neural networks, how data is passed with forward propagation, and how training works with back-propagation, leveraging open-source deep learning frameworks.","youtubeID":"btxc0DcsUxQ"}]},{"id":4,"courseTitle":null,"author":null,"courseBanner":null,"videos":[{"id":1,"videoTitle":"DTEL1 - 1 Welcome to DTE class","link":"https://www.youtube.com/watch?v=DtX1hN0FRfk&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p","description":"In this video, we go over the course overview, learning outcomes and machine learning resources that we will use in this class.","youtubeID":"DtX1hN0FRfk"}]},{"id":5,"courseTitle":"DTLE","author":"AWS Machine Learning University","courseBanner":null,"videos":[{"id":1,"videoTitle":"DTEL1 - 2 Class Introduction","link":"https://www.youtube.com/watch?v=jBC6zwyZG1g&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=2","description":"In this video, we will introduce main concepts of this class and explain the motivation to use tree-based and ensemble methods.","youtubeID":"jBC6zwyZG1g"},{"id":2,"videoTitle":"DTEL1 - 3 Introduction to Decision Trees","link":"https://www.youtube.com/watch?v=oKk68_B0nSw&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=3","description":"We will learn how to build a decision tree in this video. We will go over the general tree structure learning split and leaf nodes. Then, we will use an example dataset and build our tree from scratch.","youtubeID":"oKk68_B0nSw"},{"id":3,"videoTitle":"DTEL1 - 4 Impurity Functions","link":"https://www.youtube.com/watch?v=uzTwordIjhY&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=4","description":"In this video, we will talk about impurity. This is a key concept for building our trees. We will go over classification impurities: Gini and Entropy and regression impurities: Mean Absolute Error (MAE) and Variance. We will also explain information gain and its use when selecting optimum splits.","youtubeID":"uzTwordIjhY"},{"id":4,"videoTitle":"DTEL1 - 5 CART Algorithm","link":"https://www.youtube.com/watch?v=fO2aS375WY0&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=5","description":"Let’s introduce our tree building algorithm CART (Classification and Regression Trees). In this video, we will explain how this algorithm works and the sorting trick that allows us to speed up the process.","youtubeID":"fO2aS375WY0"},{"id":5,"videoTitle":"DTEL1 - 6 Basic Properties of Decision Trees","link":"https://www.youtube.com/watch?v=V3KbsX6HvuM&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=6","description":"In this video we will talk about the scale invariance of decision trees and we will make a time complexity analysis of our tree building algorithm.","youtubeID":"V3KbsX6HvuM"},{"id":6,"videoTitle":"DTEL1 7 Basic Regularization of Trees","link":"https://www.youtube.com/watch?v=vmZWJ4x5HoA&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=7","description":"Regularization is an important concept in machine learning. It helps overcome the overfitting problem. In this video, we will learn two types of regularization: Pre-pruning and post pruning. We also have a hands-on Jupyter notebook section where we show how to apply pre-pruning with Sklearn library.","youtubeID":"vmZWJ4x5HoA"},{"id":7,"videoTitle":"DTEL1 - 8 Sklearn Trees","link":"https://www.youtube.com/watch?v=AkU7CyCjgUA&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=8","description":"Let’s see the Sklearn interface for decision trees. Here, we will see the DecisionTreeClassifier and DecisionTreeRegressor with some default parameters.","youtubeID":"AkU7CyCjgUA"},{"id":8,"videoTitle":"DTEL1 - 9 Conclusion","link":"https://www.youtube.com/watch?v=n_4SePWF474&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=9","description":"In this video, we will summarize our first lecture. We learned decision trees, impurity concept and regularization of trees. Next time, we will explain bias-variance trade-off concept and introduce a new method: Extra Trees algorithm.","youtubeID":"n_4SePWF474"},{"id":9,"videoTitle":"DTEL2 2 1 Introduction","link":"https://www.youtube.com/watch?v=LbY_VEXrfDo&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=10","description":"In this lecture, we will learn the bias-variance trade-off and Extra Trees method. Let’s first remember what we learned previously: Tree building procedure, impurities and overfitting.","youtubeID":"LbY_VEXrfDo"},{"id":10,"videoTitle":"DTEL2 2 2 Bias variance trade off","link":"https://www.youtube.com/watch?v=Yem733qgegQ&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=11","description":"In this video, we explain the bias and variance concepts by fitting multiple models with different complexities. Next, we will look at bias-variance decomposition.","youtubeID":"Yem733qgegQ"},{"id":11,"videoTitle":"DTEL2 2 3 Bias variance Decomposition","link":"https://www.youtube.com/watch?v=FUCftGplkuk&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=12","description":"In this video, we show that our errors can be attributed to three different types of errors: Bias, variance and noise. We will decompose mean squared error and get these three terms at the end.","youtubeID":"FUCftGplkuk"},{"id":12,"videoTitle":"DTEL2 2 4 Generalizations of bias variance tradeoff","link":"https://www.youtube.com/watch?v=X8_XfMxU9Dw&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=13","description":"In this video, we discuss some generalizations of the bias-variance trade-off. We first mention applying the decomposition on different problems. Then, we show we can remove the variance due to added randomness if we use an ensemble of randomized algorithms.","youtubeID":"X8_XfMxU9Dw"},{"id":13,"videoTitle":"DTEL2 2 5 ExtraTrees Algorithm","link":"https://www.youtube.com/watch?v=1VgevfcOmyE&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=14","description":"In this video, we learn ExtraTrees method. ExtraTrees stand for Extremely Randomized Trees method. ExtraTrees is an ensemble method that allows us to add randomization to our tree building procedure by only considering a subset of randomly chosen features and split conditions. Overall, this helps prevent over-optimizing over split points.","youtubeID":"1VgevfcOmyE"},{"id":14,"videoTitle":"DTEL2 2 6 ExtraTrees with Sklearn","link":"https://www.youtube.com/watch?v=1lhdj9kNMzk&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=15","description":null,"youtubeID":"1lhdj9kNMzk"},{"id":15,"videoTitle":"DTEL2 2 7 Conclusion","link":"https://www.youtube.com/watch?v=l9bQEcQzaVA&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=16","description":"In this lecture, we saw that we can reduce overfitting and total error by adding randomness and training multiple models. Next, we introduce another ensemble method called Bagging.","youtubeID":"l9bQEcQzaVA"},{"id":16,"videoTitle":"DTEL3 3 1 Introduction","link":"https://www.youtube.com/watch?v=hKsdCRcvFls&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=17","description":"Let’s go over our third lecture topics. We introduce bootstrapping technique, bagging method and random forests.","youtubeID":"hKsdCRcvFls"},{"id":17,"videoTitle":"DTEL3 3 2 Bootstrap","link":"https://www.youtube.com/watch?v=74AB1Yplcnk&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=18","description":"In this video, we explain the bootstrap process that creates multiple synthetic datasets through sampling with replacement.","youtubeID":"74AB1Yplcnk"},{"id":18,"videoTitle":"DTEL3 3 3 Bagging","link":"https://www.youtube.com/watch?v=QFRzW83iyks&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=19","description":"In this video, we see how to apply the bootstrapping idea to create multiple datasets and train an ensembled model. We call this method “Bagging”.","youtubeID":"QFRzW83iyks"},{"id":19,"videoTitle":"DTEL3 3 4 1 Example","link":"https://www.youtube.com/watch?v=r7slLq4Tqng&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=20","description":"Let’s investigate overfitting problem with some examples. We first compare a simple decision tree and bagging method. Then, we see the effect of changing bootstrap size on ensembled models.","youtubeID":"r7slLq4Tqng"},{"id":20,"videoTitle":"DTEL3 3 5 Random Forest","link":"https://www.youtube.com/watch?v=4mA_mC-auDU&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=21","description":"In this video, we learn a popular ensemble method called “Random Forest”. Previously, with the bagging approach, we see that correlation between trees due to shared portions of bootstrapped samples creates limitations. With random forests, we decorrelate trees by considering a random subset of features for each tree.","youtubeID":"4mA_mC-auDU"},{"id":21,"videoTitle":"DTEL3 3 4 2 Example Notebook","link":"https://www.youtube.com/watch?v=4Rv2LeyHzTI&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=22","description":null,"youtubeID":"4Rv2LeyHzTI"},{"id":22,"videoTitle":"DTEL3 3 6 General Ensembling","link":"https://www.youtube.com/watch?v=Quc681iZmts&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=23","description":"Although we applied bagging using only trees, this shouldn’t stop us from trying different models. In this section, we see an example from an earlier machine learning competition.","youtubeID":"Quc681iZmts"},{"id":23,"videoTitle":"DTEL4 4 1 Introduction","link":"https://www.youtube.com/watch?v=cuuB0NZg3uc&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=24","description":"In this lecture, we will talk about some different ways of using tree-based methods. We will see proximities and feature importance's.","youtubeID":"cuuB0NZg3uc"},{"id":24,"videoTitle":"DTEL4 4 2 Proximities","link":"https://www.youtube.com/watch?v=Y8MKixE6p9Y&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=25","description":"Let’s start our discussion with proximities. We can treat random forests as non-linear transformation functions that convert input features to output features. Here, we make use of output features to use them as some type of similarity measure between data points.","youtubeID":"Y8MKixE6p9Y"},{"id":25,"videoTitle":"DTEL4 4 3 Proximities Visualizations","link":"https://www.youtube.com/watch?v=N8PmANPvgaw&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=26","description":"In this video, we see how to apply the random forest proximities. First, we introduce a dimension reduction technique called metric Multidimensional Scaling (mMDS). This technique maps produces a low dimensional representation that preserves the distances between data points. Then, we apply mMDs and Principle Component Analysis (PCA) on FashionMNIST dataset.","youtubeID":"N8PmANPvgaw"},{"id":26,"videoTitle":"DTEL4 4 4 Feature Importance's","link":"https://www.youtube.com/watch?v=xw4lMV2ruCM&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=27","description":"Let’s talk about another interesting use of tree-based methods. We can use trees to calculate importance of features. This can be useful for feature selection.","youtubeID":"xw4lMV2ruCM"},{"id":27,"videoTitle":"DTEL4 4 5 Limitations of Tree Feature Importance","link":"https://www.youtube.com/watch?v=5ezb1sfXb6Y&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=28","description":"In this video, we discuss some limitations of feature importance. It depends on tree structure and it has problems with correlated variables. We see an example for each case.","youtubeID":"5ezb1sfXb6Y"},{"id":28,"videoTitle":"DTEL4 4 6 Feature Importance's in Random Forest","link":"https://www.youtube.com/watch?v=vxDaGF4XfRs&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=29","description":"Let’s calculate feature importance's using random forests. This time, using random forest fixes the previous issues we had. Randomized trees fix the tree dependency issue and use of random feature subsets fixes the correlated variable problem.","youtubeID":"vxDaGF4XfRs"},{"id":29,"videoTitle":"DTEL4 4 7 Summary","link":"https://www.youtube.com/watch?v=kEcsfVDjuA4&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=30","description":"In this lecture, we learned proximities and feature importance's. Next, we focus on a new ensembled method called “boosting”.","youtubeID":"kEcsfVDjuA4"},{"id":30,"videoTitle":"DTEL5 5 1 Introduction","link":"https://www.youtube.com/watch?v=qxQa1-xwmX4&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=31","description":"In this video, we go over topics for this lecture: Boosting technique, gradient boosting and some well-known libraries such as XGBoost, LightGBM and CatBoost.","youtubeID":"qxQa1-xwmX4"},{"id":31,"videoTitle":"DTEL5 5 2 Boosting","link":"https://www.youtube.com/watch?v=zGp6X6rLhQ0&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=32","description":"Let’s start with the boosting technique. Boosting is an ensemble technique that adds weak models in a sequential manner. Overall, we get a powerful model out of multiple added weak learners.","youtubeID":"zGp6X6rLhQ0"},{"id":32,"videoTitle":"DTEL5 5 3 Gradient Boosting","link":"https://www.youtube.com/watch?v=vV3dKjfiT_c&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=33","description":"We used the boosting technique with a regression setting so far. How about different problems and loss functions? Here, we can take the derivative of loss function and see that negative gradient is equal to residuals. We can generalize this rule to any other loss function and use gradient boosting with different loss functions.","youtubeID":"vV3dKjfiT_c"},{"id":33,"videoTitle":"DTEL5 5 4 XGBoost","link":"https://www.youtube.com/watch?v=RGR1BL4WEe0&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=34","description":"In this video, we introduce the XGBoost method. This method uses gradient boosting with some improvements. We talk about the approximate greedy method and show the library interface with some default parameters","youtubeID":"RGR1BL4WEe0"},{"id":34,"videoTitle":"DTEL5 5 5 LightGBM","link":"https://www.youtube.com/watch?v=NsCdkzi2soE&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=35","description":"LightGBM is another important gradient boosting method. We talk about two main contributions of this method: Gradient-based One-Side Sampling and Exclusive Feature Bundling","youtubeID":"NsCdkzi2soE"},{"id":35,"videoTitle":"DTEL5 5 6 CatBoost","link":"https://www.youtube.com/watch?v=668Yj2sgQo4&list=PL8P_Z6C4GcuXrj9crYtU_XaYh3Jac4x0p&index=36","description":"Here is our last gradient boosting method: CatBoost. Main contribution of this method is the ordering principle. We introduce ordered target encoding and ordered boosting.","youtubeID":"668Yj2sgQo4"}]}]}